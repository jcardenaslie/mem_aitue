{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "personas = pd.read_csv('..\\\\Datos\\\\personas_cotizacion.csv',encoding = \"ISO-8859-1\")\n",
    "# personas = pd.read_csv(data_directory+'personas.csv')\n",
    "personas = personas.drop(personas.columns[0], axis=1)\n",
    "personas = personas.drop(['rut'], axis=1)\n",
    "# personas['negocio'] = []\n",
    "personas.head(2)\n",
    "p_negocio = personas.negocio\n",
    "personas = personas.drop('negocio',axis=1)\n",
    "print(personas.shape)\n",
    "personas = pd.get_dummies(personas)\n",
    "personas = personas.drop(['max_rango_edad'],axis=1)\n",
    "personas.shape\n",
    "personas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Max Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(X_train,y_train)\n",
    "model2.fit(X_train,y_train)\n",
    "model3.fit(X_train,y_train)\n",
    "\n",
    "pred1=model1.predict(X_test)\n",
    "pred2=model2.predict(X_test)\n",
    "pred3=model3.predict(X_test)\n",
    "\n",
    "final_pred = np.array([])\n",
    "for i in range(0,len(X_test)):\n",
    "    final_pred = np.append(final_pred, mode([pred1[i], pred2[i], pred3[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = DecisionTreeClassifier(random_state=1)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(X_train,y_train)\n",
    "model2.fit(X_train,y_train)\n",
    "model3.fit(X_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(X_test)\n",
    "pred2=model2.predict_proba(X_test)\n",
    "pred3=model3.predict_proba(X_test)\n",
    "\n",
    "finalpred=(pred1+pred2+pred3)/3\n",
    "print(finalpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(X_train,y_train)\n",
    "model2.fit(X_train,y_train)\n",
    "model3.fit(X_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(X_test)\n",
    "pred2=model2.predict_proba(X_test)\n",
    "pred3=model3.predict_proba(X_test)\n",
    "\n",
    "finalpred=(pred1*0.3+pred2*0.3+pred3*0.4)\n",
    "print(finalpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Advanced Ensemble techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Stacking\n",
    "Stacking is an ensemble learning technique that uses predictions from multiple models (for example decision tree, knn or svm) to build a new model. This model is used for making predictions on the test set. Below is a step-wise explanation for a simple stacked ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def Stacking(model, train, y, test, n_fold):\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=n_fold,random_state=1)\n",
    "    \n",
    "    test_pred = np.empty((test.shape[0],1),float)\n",
    "    train_pred = np.empty((0,1),float)\n",
    "    \n",
    "    for train_indices, val_indices in folds.split(train, y.values):\n",
    "        x_train, x_val = train.iloc[train_indices], train.iloc[val_indices]\n",
    "        y_train, y_val = y.iloc[train_indices], y.iloc[val_indices]\n",
    "\n",
    "        model.fit(X=x_train, y=y_train)\n",
    "        \n",
    "        train_pred = np.append(train_pred, model.predict(x_val))\n",
    "        test_pred = np.append(test_pred, model.predict(test))\n",
    "    \n",
    "    return test_pred.reshape(-1,1), train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "test_pred1 ,train_pred1 = Stacking(model=model1, n_fold=10, train=X_train, test=X_test, y=y_train)\n",
    "\n",
    "train_pred1=pd.DataFrame(train_pred1)\n",
    "test_pred1=pd.DataFrame(test_pred1)\n",
    "\n",
    "print(train_pred1.shape)\n",
    "print(test_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = KNeighborsClassifier()\n",
    "\n",
    "test_pred2 ,train_pred2=Stacking(model=model2,n_fold=10,train=X_train,test=X_test,y=y_train)\n",
    "\n",
    "train_pred2=pd.DataFrame(train_pred2)\n",
    "test_pred2=pd.DataFrame(test_pred2)\n",
    "\n",
    "print(train_pred2.shape)\n",
    "print(test_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([train_pred1, train_pred2], axis=1)\n",
    "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
    "\n",
    "print(df.shape, y_train.shape)\n",
    "print(df_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=1)\n",
    "model.fit(df,y_train)\n",
    "model.score(df_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Blending\n",
    "Blending follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions. In other words, unlike stacking, the predictions are made on the holdout set only. The holdout set and the predictions are used to build a model which is run on the test set. Here is a detailed explanation of the blending process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifier()\n",
    "model1.fit(X_train, y_train)\n",
    "val_pred1 = model1.predict(x_val)\n",
    "test_pred1 = model1.predict(x_test)\n",
    "val_pred1 = pd.DataFrame(val_pred1)\n",
    "test_pred1 = pd.DataFrame(test_pred1)\n",
    "\n",
    "model2 = KNeighborsClassifier()\n",
    "model2.fit(X_train,y_train)\n",
    "val_pred2 = model2.predict(x_val)\n",
    "test_pred2 = model2.predict(x_test)\n",
    "val_pred2 = pd.DataFrame(val_pred2)\n",
    "test_pred2 = pd.DataFrame(test_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_val=pd.concat([x_val, val_pred1,val_pred2],axis=1)\n",
    "df_test=pd.concat([x_test, test_pred1,test_pred2],axis=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(df_val,y_val)\n",
    "model.score(df_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Bagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Approach to handling Imbalanced Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Bagging Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "digits = load_digits()\n",
    "data = scale(digits.data)\n",
    "# X = data\n",
    "# y = digits.target\n",
    "X = personas\n",
    "y = p_negocio\n",
    "\n",
    "bagging = BaggingClassifier(LogisticRegression(C=3166), max_samples=0.2)\n",
    "scores = cross_val_score(bagging, X, y)\n",
    "\n",
    "mean= scores.mean()\n",
    "print(scores)\n",
    "print(mean)\n",
    "\n",
    "\n",
    "bagging.fit(X,y)\n",
    "y_pred = bagging.predict(X)\n",
    "\n",
    "plot_confusion_matrix(bagging, X, y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "digits = load_digits()\n",
    "data = scale(digits.data)\n",
    "X = data\n",
    "y = digits.target\n",
    "# X = personas\n",
    "# y = p_negocio\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1.0, random_state=0)\n",
    "scores = cross_val_score(clf, X, y)\n",
    "print(scores)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1, max_depth=None, min_samples_split=1.0, random_state=0)\n",
    "scores = cross_val_score(clf, X, y)\n",
    "print(scores)\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=1.0, random_state=0)\n",
    "scores = cross_val_score(clf, X, y)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.datasets.mldata import fetch_mldata\n",
    "\n",
    "n_estimators = 400\n",
    "learning_rate = 1\n",
    "\n",
    "# heart = fetch_mldata('heart')\n",
    "# X = heart.data\n",
    "# y = np.copy(heart.target)\n",
    "# y[y==-1] = 0\n",
    "\n",
    "# X_test, y_test = X[189:], y[189:]\n",
    "# X_train, y_train = X[:189], y[:189]\n",
    "\n",
    "# X = personas\n",
    "# y = p_negocio\n",
    "\n",
    "dt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)\n",
    "dt_stump.fit(X_train, y_train)\n",
    "dt_stmp_err = 1.0 - dt_stump.score(X_test, y_test)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_err = 1.0 - dt.score(X_test, y_test)\n",
    "\n",
    "# n_estimators = number of weak learners\n",
    "# base_estimator use to define other weak learners\n",
    "ada_discrete = AdaBoostClassifier(base_estimator=dt_stump, learning_rate=learning_rate, \n",
    "                                  n_estimators=n_estimators, algorithm='SAMME')\n",
    "ada_discrete.fit(X_train, y_train)\n",
    "scores = cross_val_score(ada_discrete, X_test, y_test)\n",
    "print(scores)\n",
    "means = scores.mean()\n",
    "print(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n",
    "# License: MIT\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# iris = load_iris()\n",
    "# X, y = make_imbalance(iris.data, iris.target, ratio={0: 25, 1: 40, 2: 50},\n",
    "#                       random_state=0)\n",
    "\n",
    "X = personas\n",
    "y = p_negocio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "bagging = BaggingClassifier(random_state=0)\n",
    "balanced_bagging = BalancedBaggingClassifier(random_state=0)\n",
    "\n",
    "print('Class distribution of the training set: {}'.format(Counter(y_train)))\n",
    "\n",
    "bagging.fit(X_train, y_train)\n",
    "balanced_bagging.fit(X_train, y_train)\n",
    "\n",
    "print('Class distribution of the test set: {}'.format(Counter(y_test)))\n",
    "\n",
    "print('Classification results using a bagging classifier on imbalanced data')\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "print(classification_report_imbalanced(y_test, y_pred_bagging))\n",
    "cm_bagging = confusion_matrix(y_test, y_pred_bagging)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_bagging, classes=[0,1],\n",
    "                      title='Confusion matrix using BaggingClassifier')\n",
    "\n",
    "print('Classification results using a bagging classifier on balanced data')\n",
    "y_pred_balanced_bagging = balanced_bagging.predict(X_test)\n",
    "print(classification_report_imbalanced(y_test, y_pred_balanced_bagging))\n",
    "cm_balanced_bagging = confusion_matrix(y_test, y_pred_balanced_bagging)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_balanced_bagging, classes=[0,1],\n",
    "                      title='Confusion matrix using BalancedBaggingClassifier')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Authors: Christos Aridas\n",
    "#          Guillaume Lemaitre <g.lemaitre58@gmail.com>\n",
    "# License: MIT\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "from sklearn import datasets, neighbors\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "LW = 2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "class DummySampler(object):\n",
    "\n",
    "    def sample(self, X, y):\n",
    "        return X, y\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def fit_sample(self, X, y):\n",
    "        return self.sample(X, y)\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3)\n",
    "\n",
    "# Load the dataset\n",
    "data = datasets.fetch_lfw_people()\n",
    "majority_person = 1871  # 530 photos of George W Bush\n",
    "minority_person = 531  # 29 photos of Bill Clinton\n",
    "majority_idxs = np.flatnonzero(data.target == majority_person)\n",
    "minority_idxs = np.flatnonzero(data.target == minority_person)\n",
    "idxs = np.hstack((majority_idxs, minority_idxs))\n",
    "\n",
    "X = data.data[idxs]\n",
    "y = data.target[idxs]\n",
    "y[y == majority_person] = 0\n",
    "y[y == minority_person] = 1\n",
    "\n",
    "classifier = ['3NN', neighbors.KNeighborsClassifier(3)]\n",
    "\n",
    "samplers = [\n",
    "    ['Standard', DummySampler()],\n",
    "    ['ADASYN', ADASYN(random_state=RANDOM_STATE)],\n",
    "    ['ROS', RandomOverSampler(random_state=RANDOM_STATE)],\n",
    "    ['SMOTE', SMOTE(random_state=RANDOM_STATE)],\n",
    "]\n",
    "\n",
    "pipelines = [\n",
    "    ['{}-{}'.format(sampler[0], classifier[0]),\n",
    "     make_pipeline(sampler[1], classifier[1])]\n",
    "    for sampler in samplers\n",
    "]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for name, pipeline in pipelines:\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    for train, test in cv.split(X, y):\n",
    "        probas_ = pipeline.fit(X[train], y[train]).predict_proba(X[test])\n",
    "        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    mean_tpr /= cv.get_n_splits(X, y)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, linestyle='--',\n",
    "             label='{} (area = %0.2f)'.format(name) % mean_auc, lw=LW)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=LW, color='k',\n",
    "         label='Luck')\n",
    "\n",
    "# make nice plotting\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()\n",
    "ax.get_yaxis().tick_left()\n",
    "ax.spines['left'].set_position(('outward', 10))\n",
    "ax.spines['bottom'].set_position(('outward', 10))\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
